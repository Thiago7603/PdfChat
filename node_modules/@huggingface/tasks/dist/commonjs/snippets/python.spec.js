"use strict";
Object.defineProperty(exports, "__esModule", { value: true });
const vitest_1 = require("vitest");
const python_js_1 = require("./python.js");
(0, vitest_1.describe)("inference API snippets", () => {
    (0, vitest_1.it)("conversational llm", async () => {
        const model = {
            id: "meta-llama/Llama-3.1-8B-Instruct",
            pipeline_tag: "text-generation",
            tags: ["conversational"],
            inference: "",
        };
        const snippet = (0, python_js_1.getPythonInferenceSnippet)(model, "api_token");
        (0, vitest_1.expect)(snippet[0].content).toEqual(`from huggingface_hub import InferenceClient

client = InferenceClient(api_key="api_token")

messages = [
	{
		"role": "user",
		"content": "What is the capital of France?"
	}
]

stream = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct", 
	messages=messages, 
	max_tokens=500,
	stream=True
)

for chunk in stream:
    print(chunk.choices[0].delta.content, end="")`);
    });
    (0, vitest_1.it)("conversational llm non-streaming", async () => {
        const model = {
            id: "meta-llama/Llama-3.1-8B-Instruct",
            pipeline_tag: "text-generation",
            tags: ["conversational"],
            inference: "",
        };
        const snippet = (0, python_js_1.getPythonInferenceSnippet)(model, "api_token", { streaming: false });
        (0, vitest_1.expect)(snippet[0].content).toEqual(`from huggingface_hub import InferenceClient

client = InferenceClient(api_key="api_token")

messages = [
	{
		"role": "user",
		"content": "What is the capital of France?"
	}
]

completion = client.chat.completions.create(
    model="meta-llama/Llama-3.1-8B-Instruct", 
	messages=messages, 
	max_tokens=500
)

print(completion.choices[0].message)`);
    });
    (0, vitest_1.it)("conversational vlm", async () => {
        const model = {
            id: "meta-llama/Llama-3.2-11B-Vision-Instruct",
            pipeline_tag: "image-text-to-text",
            tags: ["conversational"],
            inference: "",
        };
        const snippet = (0, python_js_1.getPythonInferenceSnippet)(model, "api_token");
        (0, vitest_1.expect)(snippet[0].content).toEqual(`from huggingface_hub import InferenceClient

client = InferenceClient(api_key="api_token")

messages = [
	{
		"role": "user",
		"content": [
			{
				"type": "text",
				"text": "Describe this image in one sentence."
			},
			{
				"type": "image_url",
				"image_url": {
					"url": "https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg"
				}
			}
		]
	}
]

stream = client.chat.completions.create(
    model="meta-llama/Llama-3.2-11B-Vision-Instruct", 
	messages=messages, 
	max_tokens=500,
	stream=True
)

for chunk in stream:
    print(chunk.choices[0].delta.content, end="")`);
    });
    (0, vitest_1.it)("text-to-image", async () => {
        const model = {
            id: "black-forest-labs/FLUX.1-schnell",
            pipeline_tag: "text-to-image",
            tags: [],
            inference: "",
        };
        const snippets = (0, python_js_1.getPythonInferenceSnippet)(model, "api_token");
        (0, vitest_1.expect)(snippets.length).toEqual(2);
        (0, vitest_1.expect)(snippets[0].client).toEqual("huggingface_hub");
        (0, vitest_1.expect)(snippets[0].content).toEqual(`from huggingface_hub import InferenceClient
client = InferenceClient("black-forest-labs/FLUX.1-schnell", token="api_token")

# output is a PIL.Image object
image = client.text_to_image("Astronaut riding a horse")`);
        (0, vitest_1.expect)(snippets[1].client).toEqual("requests");
        (0, vitest_1.expect)(snippets[1].content).toEqual(`import requests

API_URL = "https://api-inference.huggingface.co/models/black-forest-labs/FLUX.1-schnell"
headers = {"Authorization": "Bearer api_token"}

def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.content
image_bytes = query({
	"inputs": "Astronaut riding a horse",
})

# You can access the image with PIL.Image for example
import io
from PIL import Image
image = Image.open(io.BytesIO(image_bytes))`);
    });
});
